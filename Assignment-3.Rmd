---
title: "HW3 04.06.2022"
author: "Arpan Chatterji, Rajsitee Dhavale"
date: "4/6/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load-packages, include = FALSE}
library(tidyverse)
library(ggplot2)
library(mosaic)
library (knitr)
library(caret)
library(foreach)
library(FNN)
library(rsample)
library(modelr)
library(class)
library(quantmod)
```

## What causes what?

``` 
1.
You cannot just get data from a few different cities and run the regression of crime on police to understand how more cops in the streets affect crime. This is because you need an example of a city where the number of (increase in) the police force is unrelated to crime levels in the city. Only if you have this will you have an effective control group, thus giving you accurate unbiased results for the policy question “if you hire more cops, will crime go down?”
Most cities tend to have a direct relationship between the number of police and the crime level because cities with high levels of crime have a higher incentive to hire more police. 

2. The researchers wanted a clever way to establish a causal relationship between more police and less crime. They were looking for a natural experiment. 

The approach they used was as follows: They looked for a city with a large number of police for reasons unrelated to crime. Thus, they picked Washington D.C. as it had a terrorism alert system (with color-coded alerts). Here, people would relate the increase in the police force to a potential terror threat and not to an increase in crime. Thus, people (tourists i.e., potential victims of criminals) and criminals wouldn’t alter their behavior. The researchers then checked Metro ridership data to see if fewer tourists had ventured out (fewer victims) and found that ridership was largely unchanged. 

Results: 
For (1): Only One Variable used 
When there is 1 more cop on the street, the crime rate falls by 7.316 units (inverse relationship), all other things constant. 
The R squared number 0.14 indicates that 14% of the data fit the model. However, this by itself doesn’t explain the causation relationship between the independent and dependent variables. 

For (2): Control for Metro Ridership
Where there is one more police on the street, crime falls by 6.316 units. Here the coefficient is significant at 1%, indicating better results as 1% level of significance lowers the chance of a false positive, all other things constant. This further proves the hypothesis that increasing the police force in a city lowers the level of crime. The R squared number 0.17 indicates that 17% of the data fit the model. However, this by itself doesn’t explain the causation relationship between the independent and dependent variables. 

Overall, the table indicates that holding everything else constant, an increase in the number of police leads to a decline in crime levels. 

3. The researchers had to control for Metro ridership because they wanted to check whether there were any other reasons for the crime rate to fall. For example, did the crime rate fall because there were fewer victims on the street due to the terror threat? 

4. In table 4, the researchers are trying to estimate whether the effect of high alerts days on crime was the same across all parts of the city. This is a more refined analysis than the one in table 2. When the researchers used interactions between high alert days and location, they found the effect to be clear only in District 1. This is probably because that part of the city has the highest number of important monuments and so is the highest on the terrorism alert. Thus, this area will also have the highest number of cops. 
The effect in the other districts is still negative, indicating that increasing the number of cops does lower crime, but the effect is very small. When we look at this and then at the standard error in the brackets, we could conclude the effect to be zero too because of the 5% level of significance. 
```
## Tree Modeling Dengue Cases


```{r, warning=FALSE, error=FALSE, include=TRUE}
library(knitr)
library(readr)
library(rmarkdown)
knitr::opts_chunk$set(echo=F, message=FALSE, warning = FALSE)
# Imported libraries
install.packages("kableExtra", dependencies = TRUE)
install.packages("gbm", dependencies = TRUE)

library(tidyverse)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(rsample)
library(randomForest)
library(lubridate)
library(modelr)
library("devtools")
library(gbm)
library(kableExtra)
```

```{r, echo=FALSE, warning=FALSE, error=FALSE}
dengue <- read_csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/dengue.csv") %>% drop_na()
```

```{r, echo=FALSE, warning=FALSE, error=FALSE}
## Tree Model: Dengue Cases
lapply(dengue, class)

dengue$city <- dengue$city %>% factor()
dengue$season <- dengue$season %>% factor()

# Train-Test Split
dengue_split <- initial_split(dengue, 0.8)
dengue_train <- training(dengue_split)
dengue_test <- testing(dengue_split)
```

```{r, echo=FALSE, warning=FALSE, error=FALSE}
# CART (Classification & Regression Trees)
cart_dengue <- rpart(total_cases ~ season + specific_humidity + precipitation_amt + tdtr_k,
                     data = dengue_train, control = rpart.control(cp = 0.002, minsplit = 30))

# Plot
plotcp(cart_dengue, main = "Cross_Validated Error by CP")
```

```{r, echo=FALSE, warning=FALSE, error=FALSE}
# Pick the smallest tree whose CV error is within 1 std error of the minimum
cp_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  cp_opt
}

cp_1se(cart_dengue)

# Pruning the tree at that level
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

cart_dengue_prune <- prune_1se(cart_dengue)
```

## Random Forest Model
```{r, echo=FALSE, warning=FALSE, error=FALSE}
# Random Forest Model
dengue_randomforest <- randomForest(total_cases ~ season + specific_humidity + precipitation_amt + tdtr_k,
                              data = dengue_train, importance = TRUE, na.action = na.omit)

# Out-of-bag MSE as a function of the number of trees used (we don't use the test set here)gh
plot(dengue_randomforest, main = "Out-of-Bag MSE by No. of Trees")

# Gradient Boosting
dengue_gradient_boosting <- gbm(total_cases ~ season + specific_humidity + tdtr_k + precipitation_amt,
                                data = dengue_train, distribution = "gaussian",n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

# Error curve
gbm.perf(dengue_gradient_boosting)
```

```{r, echo=FALSE, warning=FALSE, error=FALSE}
# RMSE
modelr::rmse(cart_dengue, dengue_test)
modelr::rmse(cart_dengue_prune, dengue_test)
modelr::rmse(dengue_randomforest, dengue_test)
modelr::rmse(dengue_gradient_boosting, dengue_test)

rmsemodels <- c("RMSE CART" = rmse(cart_dengue, dengue_test),
                "RMSE CART Prune" = rmse(cart_dengue_prune, dengue_test),
                "RMSE Random Forest" = rmse(dengue_randomforest, dengue_test),
                "RMSE Gradient Boosting" = rmse(dengue_gradient_boosting, dengue_test))

kable(rmsemodels, col.names = c("RMSE"), caption = "*Table 2.1 RMSE per Model*", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

## Final Result
```{r, echo=FALSE, warning=FALSE, error=FALSE}
# Final Result: We now see that the Random Forest Model is the best
partialPlot(dengue_randomforest, as.data.frame(dengue_test), 'specific_humidity', las = 1)
partialPlot(dengue_randomforest, as.data.frame(dengue_test), 'precipitation_amt', las = 1)
partialPlot(dengue_randomforest, as.data.frame(dengue_test), 'season', las = 1, ) #(Diagram: Bar graph)
```


## 3) Predictive model building: green certification 

The goal is to build the best predictive model possible for revenue per square foot per calendar year, and to use this model to quantify the average change in rental income per square foot associated with green certification, controlling for other features of the building.


```{r include=FALSE}
rm(list = ls())
greenbuildings <- read.csv("~/Desktop/Data Mining/data/greenbuildings.csv")
greenbuildings = greenbuildings %>%
  mutate(
    revenue = Rent * leasing_rate
  )
## Split the data for testing
greenbuildings_initial = initial_split(greenbuildings)
n = nrow(greenbuildings)
greenbuildings_train = training(greenbuildings_initial)
greenbuildings_test = testing(greenbuildings_initial)
# What is more relevant for green rating?
forest = randomForest(revenue ~ . - Rent - leasing_rate - green_rating,
                      na.action = na.omit,
                      data=greenbuildings_train)
greenbuildings_casetest = predict(forest, greenbuildings_test)
plot(greenbuildings_casetest, greenbuildings_test$revenue)
rmseforest = rmse(forest, greenbuildings_test)
plot(forest)
```
### Methods  
  
For this report, the goal was to detect the change in rent on houses with a green certificate, such that an architect would choose to construct if there is more revenue that can be generated when renting a "green" house. To create a predictive model, we created a variable for the rental income per square foot, or the average rent per square foot multiplied by the percentage of occupancy of the house. Following this, we had to define whether it was more relevant to include the variables for green rating as two separate controls or whether to remove those variables and use the general "green_rating" variable. To do so, we ran two different random forests, one with both variables and one with the general variable only.  
  

```{r echo=FALSE}
varImpPlot(forest)
```
  
  
```{r include=FALSE}
forest1 = randomForest(revenue ~. - Rent - leasing_rate - LEED - Energystar,
                      na.action = na.omit,
                      data=greenbuildings_train)
greenbuildings_casetest = predict(forest1, greenbuildings_test)
plot(greenbuildings_casetest, greenbuildings_test$revenue)
rmseforest1 = rmse(forest1, greenbuildings_test)
plot(forest1)
```
  
  
```{r echo=FALSE}
varImpPlot(forest1)
```
  
With those graphs, we saw that there was no real difference between the set of variables to use, so we decided to create four different predictive models using the general green rating instead of two separate models.  
  
```{r include=FALSE}
# After choosing best variable, compare to boosted
boost = gbm(revenue ~. - Rent - leasing_rate - LEED - Energystar, 
             data = greenbuildings_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
gbm.perf(boost)
yhat_test_gbm = predict(boost, greenbuildings_test, n.trees=350)
rmsegradient = rmse(boost, greenbuildings_test)
# Compare to linears
lm2 = lm(revenue ~. - Rent - leasing_rate - LEED - Energystar, data=greenbuildings_train)
lm3 = lm(revenue ~ (. - Rent - leasing_rate - LEED - Energystar)^2, data=greenbuildings_train)
rmsemedium = rmse(lm2, greenbuildings_test)
rmselarge = rmse(lm3, greenbuildings_test)
```
  
```{r echo=FALSE} 
tab1 <- matrix(c(rmseforest1, rmsegradient, rmsemedium, rmselarge), ncol=1, byrow=TRUE)
colnames(tab1) <- c("RMSE")
rownames(tab1) <- c("Random Forest", "Boosted Forest", "Medium Model", "Large Model")
tab1 <- as.table(tab1)
kable(tab1, align="lr", caption="Table with prediction model's RMSE")
```
  
The results, which are shown on the above table convinced us to use the random forest model to predict the rental income per square foot, since it had the smallest RMSE.  
  
```{r echo=FALSE}
p4 = pdp::partial(forest1, pred.var = 'green_rating')
ggplot(p4) +
  geom_line(mapping=aes(x=green_rating, y=yhat))
```
  
### Conclusion  
  
After predicting the value with our model, we decided to graph the average rental income per square foot associated with the green certificate. The graph shows that, on average, there is approximately $50-60 difference between having a green certificate or not. That means it is not very significant on the rental income the house having a green certificate.  


## Question 4 - Predictive model building: California housing  
``` {r include=FALSE}
rm(list = ls())
CAhousing <- read.csv("~/Desktop/Data Mining/data/CAhousing.csv")
########## Calculating average variables and getting map of California
CAhousing = CAhousing %>%
mutate(averagerooms = totalRooms/households,
       averagebedrooms = totalBedrooms/households,
       averagetenants = population/households)
ca_space <- c(left = -125, right = - 113, top = 42, bottom = 32)
```

``` {r include=FALSE}
########## Splitting data
CAhousing_initial = initial_split(CAhousing)
n = nrow(CAhousing)
CAhousing_train = training(CAhousing_initial)
CAhousing_test = testing(CAhousing_initial)
```

``` {r include=FALSE}
########## Tree with averages
forest1 = randomForest(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants,
                       na.action = na.omit,
                       data=CAhousing_train)
rfmean = rmse(forest1, CAhousing_test)
########## Tree with total
forest2 = randomForest(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population,
                       na.action = na.omit,
                       data=CAhousing_train)
rftotal = rmse(forest2, CAhousing_test)
########## Boosted with averages
boost1 = gbm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, 
             data = CAhousing_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
gbm.perf(boost1)
boostmean = rmse(boost1, CAhousing_test)
########## Boosted with total
boost2 = gbm(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population, 
             data = CAhousing_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
gbm.perf(boost2)
boosttotal = rmse(boost2, CAhousing_test)
```

```{r include=FALSE}
########## regressions predictions with average
lm2 = lm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, data=CAhousing_train)
lm3 = lm(medianHouseValue ~ (medianIncome + households + averagerooms + averagebedrooms +  averagetenants)^2, data=CAhousing_train)
medmean = rmse(lm2, CAhousing_test)
largemean = rmse(lm3, CAhousing_test)
########## regressions predictions with totals
lm4 = lm(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population, data=CAhousing_train)
lm5 = lm(medianHouseValue ~ (medianIncome + households + totalRooms + totalBedrooms +  population)^2, data=CAhousing_train)
medtotal = rmse(lm4, CAhousing_test)
largetotal = rmse(lm5, CAhousing_test)
```
  
  
When we were creating a predictive model, we first thought about what variables to include in the model, whether we wanted to use them all or not. In addition to that we had to decided whether to use average or total values for some of the variables, given that rooms, people and bedrooms were given for the total of the area, so we decided that we wanted to check if it was more relevant to use a total or an average per household in the same area. To do so, we conducted the same medium and large regressions, as well as random forests and gradient-boosted forests to check for the smallest RMSE.  
  
```{r echo=FALSE}
######### Table of RMSEs
tab <- matrix(c(rfmean, rftotal, boostmean, boosttotal, medmean, medtotal, largemean, largetotal), ncol=1, byrow=TRUE)
colnames(tab) <- c("RMSE")
rownames(tab) <- c("RandomForest - Mean", "RandomForest - Total", "Boosted - Mean", "Boosted - Total", "Medium - Mean", "Medium - Total", "Large - Mean", "Large - Total")
tab <- as.table(tab)
kable(tab, align="lr", caption="Table with prediction model's RMSE")
```
  
The results of the table showed us that the lowest RMSE was from the gradient-boosted forest with the average of some of the variables. So we use it to predict the median house value of the houses in California and we subtracted those predictions from the actual values to get the errors of the model.  
  
```{r include=FALSE}
########## chose best after comparing
boost3 = gbm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, 
             data = CAhousing,
             interaction.depth=4, n.trees=500, shrinkage=.05)
test = predict(boost3, CAhousing)
CAhousing = CAhousing %>%
  mutate(prediction1 = test,
         residuals1 =  medianHouseValue - prediction1)
```

```{r echo=FALSE}
# Plots
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = medianHouseValue))
```
  
In this graph, we see the actual median house value for different areas of California.  
  
```{r echo=FALSE}
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = prediction1))
```
  
This graph shows the predictions of median house values made by our gradient-boosted forest model for houses across different areas of California.  
  
```{r echo=FALSE}
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = residuals1))
```
  
This last graph is the errors of our predictive model, in each area of California, it shows how close to the actual value our prediction was.   


## Modeling Approach

### Linear Model (Baseline)

In order to have a baseline to compare other models to, I performed a multiple regression on all the features in the training set. The RMSE of predicting the linear model on the test set is shown below.

````{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
ca_housing_lm <- lm(medianHouseValue ~ . , data=ca_housing_train)
# fit a linear model to the training set
ca_housing_rmse_sim = do(10)*{
  ca_housing_split = initial_split(ca_housing, prop = 0.8)
  ca_housing_train = training(ca_housing_split)
  ca_housing_test = testing(ca_housing_split)
  ca_housing_lm = update(ca_housing_lm, data=ca_housing_train)
  
  ca_housing_model_errors = c(rmse(ca_housing_lm, ca_housing_test))
  
}
# Average performance across the splits
 ca_housing_rmse_means_lm <- colMeans(ca_housing_rmse_sim)
 ca_housing_rmse_means_lm
```
### Random Forest
I utilized all of the features in the training set and calculated the resulting RMSE between the predcited revenue from the random forest model and the actual revenue from the test set which is shown below. It can be seen that the random forest model significantly outperforms the linear model based on the RMSE difference between the two (~18000).
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# fit a random forest model on the training set
ca_housing_random_forest_fit = randomForest(medianHouseValue ~ .,
                           data=ca_housing_train, importance = TRUE)
# calculate rmse of the model on the test set
rmse(ca_housing_random_forest_fit, ca_housing_test)
```
Below is the variable importance plot of the random forests model. Observing the scale of the x-axis, it can be concluded that all of these variables are important in building the model. Hence, I will keep the model as is. 
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
vi1 = varImpPlot(ca_housing_random_forest_fit, type=1)
```
### Gradient-Boosted Trees (Model of Choice)
I used all of the features in the training set and assumed a Gaussian distibution for the gradient boosted model with 10-fold cross-validation. The parameters for this model are as follows:  interaction.depth = 4, n.trees = 3000, shrinkage = .05. I ended up choosing such a high number of trees becuase the loss function kept showing the n-th tree as the mean-squared-error minimizer until I chose 3,000 and received a 2,900+ number as the mean-squared-error minimizer. The number of trees that minimized the mean-squared-error and a plot of the loss function is displayed below. Additionally, I manually cross-validated the interaction depth and shrinkage based on the lowest RMSE measure and arrived at the chosen parameters. 
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# build boosted regression trees with gaussian distribution and 10-fold cv
ca_boost <- gbm(medianHouseValue ~ ., 
               data = ca_housing_train,
               distribution = "gaussian",
               cv.folds = 10,
               interaction.depth=4, n.trees=3000, shrinkage=.05)
# plot loss function as a result of n trees added to the ensemble
gbm.perf(ca_boost, method = "cv")
```
The RMSE between the predicted revenue and the actual revenue from the test set is displayed below. It is lower than that of the random forest model. Therefore, this model will be the model of choice.
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# predict boost on test set to obtain yhat
ca_yhat_test_gbm = predict(ca_boost, ca_housing_test, n.trees=3000)
# RMSE between yhat and y from the test set
RMSE(ca_yhat_test_gbm , ca_housing_test$medianHouseValue)
```
## Map Plots
I used the gradient boosted model above to build three map plots shown below.
### Actual Median House Values
The distribution of the actual median house values is shown below on the California map. The values increase from green to red on the color scale. 
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# plot the original data, using a color scale to show medianHouseValue versus longitude (x) and latitude (y).
which_state <- "california"
county_info <- map_data("county", region=which_state)
base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
 geom_polygon(color = "black", fill = "white") +
  coord_quickmap() +
  theme_void() 
data_map <- base_map +
  geom_point(data = ca_housing, aes(x=longitude, y=latitude, color=medianHouseValue, group=medianHouseValue)) +
  scale_color_gradient(low="green", high="red") +
  labs(title = str_wrap("A scatterplot of the actual Median House Values in California from the data set", 60),
       color = "Median House Values") 
data_map
```
### Predicted Median House Values
The distribution of the gbm predicted median house values is shown below on the California map. The values increase from green to red on the color scale. The color distribution is quite similiar to that of the actual values, with the exception of high values. It seems that my model did not predict the high values as high as they should have been since mostly orange data points are observed in place of red ones. 
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
pred_ca_housing <- ca_housing %>%
  modelr::add_predictions(ca_boost)
# plot gbm's predictions of medianHouseValue versus longitude (x) and latitude (y)
pred_map <- base_map +
  geom_point(data = pred_ca_housing, aes(x=longitude, y=latitude, color=pred, group=medianHouseValue)) +
  scale_color_gradient(low="green", high="red") +
  labs(title = str_wrap("A scatterplot of predicted Median House Values in California using the gradient boosting model", 60),
              color = "Predicted Median House Values")
pred_map
```
### Residuals
The distribution of the residuals from the gbm model is shown below on the California map. The resisduals were calculated as differences between predicted median house values and actual median house values. Blue-ish points represent negative differences, green points represent minimal to no differences, and red-ish points represent positive differences. It can be seen that most of the data points are in the green range, which reflects the success of the gbm model in predicting median house values. 
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
pred_ca_housing <- pred_ca_housing %>%
  mutate(residuals=(pred-medianHouseValue))
# plot gbm's predictions of medianHouseValue versus longitude (x) and latitude (y)
pred_map <- base_map +
  geom_point(data = pred_ca_housing, aes(x=longitude, y=latitude, color=residuals, group=medianHouseValue)) +
  scale_color_gradient2(low="blue", mid="green", high="red") +
  labs(title = str_wrap("A scatterplot of residuals from the gradient boosting model", 60),
              color = "Residuals")
pred_map
```

The Lasso model gives us the variance-bias trade off. When we observe lambda to be high, the variance decreases but the bias increases.  

In the graph we observe a wide range of MSE of values for lambda that give us similar errors. Log lambda represents the penalizing factor for the sum of absolute values of coefficients. We obtain the optimal log lambda value by repeating the cross-validation. 


 Third model: Tree-Based Models
 Bagging
 
```{r, warning=FALSE, echo=FALSE}
CAhousing_split =  initial_split(CAhousing, prop=0.8)
CAhousing_train = training(CAhousing_split)
CAhousing_test  = testing(CAhousing_split)
```

 Random Forrest
 
```{r, warning=FALSE, echo=FALSE}
CAhousing <- na.omit(CAhousing)
set.seed(1)
CAhousingRandomForest = randomForest(logMedVal ~ medianIncome + latitude  + longitude + AveOccupancy + 
                                       AveRooms:AveOccupancy+latitude:longitude+medianIncome:AveRooms, data = CAhousing_train,
                                     mtry=3, importance=TRUE)
yhat_CAhousingRandomForest = predict(CAhousingRandomForest, newdata = CAhousing_test)
plot(yhat_CAhousingRandomForest, CAhousing_test$logMedVal, xlab = "Predicted Values for logMedVal: Random Forest", ylab = "logMedVal")
```

The plot shows the random forest model prediction accuracy. 


Comparison of the 3 Predictive Models: Hand-Built Linear Model, Forward Selection, Lasso, Bagging and Random Forest

  Out of sample prediction
  
```{r, warning=FALSE, echo=FALSE}
MSE <- list(HB=NULL, LASSO=NULL, RANFOR=NULL)
for(i in 1:10){
  train <- sample(1:nrow(CAhousing), 5000)
  
  lin <- cv.glm(x=Original[train,], y=logMedVal[train], lmr=1e-4)
  yhat.lin <- drop(predict(lin, Original[-train,], select="min"))
  MSE$LASSO <- c( MSE$LASSO, var(logMedVal[-train] - yhat.lin))
  
  rf <- ranger(logMedVal ~ ., data=CAhousing[train,], 
               num.tree=200, min.node.size=25, write.forest=TRUE)
  yhat.rf <- predict(rf, data=CAhousing[-train,])$predictions
  MSE$RANFOR <- c( MSE$RANFOR, var(logMedVal[-train] - yhat.rf) )
  
  Hand_Built <- ranger(logMedVal ~ ., data=CAhousing[train,], 
                       num.tree=200, min.node.size=25, write.forest=TRUE)
  yhat.hb <- predict(Hand_Built, data=CAhousing[-train,])$predictions
  MSE$HB <- c( MSE$HB, var(logMedVal[-train] - yhat.hb) )
  
  cat(i)
} 
par(mai=c(.8,.8,.1,.1))
boxplot(log(as.data.frame(MSE)), col="pink", xlab="model", ylab="log(MSE)")
```

Although each time I run the code I get a different result of the boxplots due to different training and testing data sets, the most common result is that the Random Forest Model gives us the lowest log(MSE). 
The random forest model bootstraps on the training samples. This model does not consider all variables in each split but rather selects a set of these variables for the tree split. 

First plot: Original Data

```{r, warning=FALSE, echo=FALSE}
ggplot(data=CAhousing)+ geom_point(mapping= aes(x = longitude, y = latitude, 
                                                color = logMedVal), alpha = 5) +
  theme(plot.title = element_text(hjust = 0.5)) +
 scale_color_distiller(palette = "Spectral") +
  labs(title = "California Housing",
       x = "Longitude", y = "Latitude",
       color = "Median House Value")
```


```{r,results='hide', warning=FALSE, echo=FALSE}
options(tigris_use_cache = TRUE)
tract<- tracts(state= "CA", cb= TRUE, refresh= TRUE)
```

```{r, warning=FALSE, echo=FALSE}
CAhousing$pred<-predict(CAhousingRandomForest, CAhousing)
CAhousing$error<- abs(CAhousing$logMedVal - CAhousing$pred)
CAhousing$pred= CAhousing$pred
CAhousing$error= CAhousing$error
```

Second Plot: Model's predictions

```{r, warning=FALSE, echo=FALSE}
ggplot(data= tract)+
  geom_sf()+
  geom_point(data=CAhousing, aes(x=longitude, y=latitude, color= pred/1000))+
  coord_sf()+  theme_minimal()+ 
  labs(title= "Predicted Median House Value", color= "Predicted Median House Values") 
```

Third Plot: Model's error's/residuals

```{r, warning=FALSE, echo=FALSE}
ggplot(data= tract)+
  geom_sf()+
  geom_point(data=CAhousing, aes(x=longitude, y=latitude, color= error/1000))+
  coord_sf()+
  theme_minimal()+ labs(title= "Residuals from Predicted Median House Value", color= "Residuals") 
```

``` {r include=FALSE}
########## Splitting data
CAhousing_initial = initial_split(CAhousing)
n = nrow(CAhousing)
CAhousing_train = training(CAhousing_initial)
CAhousing_test = testing(CAhousing_initial)
```

``` {r include=FALSE}
########## Tree with averages
forest1 = randomForest(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants,
                       na.action = na.omit,
                       data=CAhousing_train)
rfmean = rmse(forest1, CAhousing_test)
########## Tree with total
forest2 = randomForest(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population,
                       na.action = na.omit,
                       data=CAhousing_train)
rftotal = rmse(forest2, CAhousing_test)
########## Boosted with averages
boost1 = gbm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, 
             data = CAhousing_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
gbm.perf(boost1)
boostmean = rmse(boost1, CAhousing_test)
########## Boosted with total
boost2 = gbm(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population, 
             data = CAhousing_train,
             interaction.depth=4, n.trees=500, shrinkage=.05)
gbm.perf(boost2)
boosttotal = rmse(boost2, CAhousing_test)
```

```{r include=FALSE}
########## regressions predictions with average
lm2 = lm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, data=CAhousing_train)
lm3 = lm(medianHouseValue ~ (medianIncome + households + averagerooms + averagebedrooms +  averagetenants)^2, data=CAhousing_train)
medmean = rmse(lm2, CAhousing_test)
largemean = rmse(lm3, CAhousing_test)
########## regressions predictions with totals
lm4 = lm(medianHouseValue ~ medianIncome + households + totalRooms + totalBedrooms +  population, data=CAhousing_train)
lm5 = lm(medianHouseValue ~ (medianIncome + households + totalRooms + totalBedrooms +  population)^2, data=CAhousing_train)
medtotal = rmse(lm4, CAhousing_test)
largetotal = rmse(lm5, CAhousing_test)
```
  
  
When we were creating a predictive model, we first thought about what variables to include in the model, whether we wanted to use them all or not. In addition to that we had to decided whether to use average or total values for some of the variables, given that rooms, people and bedrooms were given for the total of the area, so we decided that we wanted to check if it was more relevant to use a total or an average per household in the same area. To do so, we conducted the same medium and large regressions, as well as random forests and gradient-boosted forests to check for the smallest RMSE.  
  
```{r echo=FALSE}
######### Table of RMSEs
tab <- matrix(c(rfmean, rftotal, boostmean, boosttotal, medmean, medtotal, largemean, largetotal), ncol=1, byrow=TRUE)
colnames(tab) <- c("RMSE")
rownames(tab) <- c("RandomForest - Mean", "RandomForest - Total", "Boosted - Mean", "Boosted - Total", "Medium - Mean", "Medium - Total", "Large - Mean", "Large - Total")
tab <- as.table(tab)
kable(tab, align="lr", caption="Table with prediction model's RMSE")
```
  
The results of the table showed us that the lowest RMSE was from the gradient-boosted forest with the average of some of the variables. So we use it to predict the median house value of the houses in California and we subtracted those predictions from the actual values to get the errors of the model.  
  
```{r include=FALSE}
########## chose best after comparing
boost3 = gbm(medianHouseValue ~ medianIncome + households + averagerooms + averagebedrooms +  averagetenants, 
             data = CAhousing,
             interaction.depth=4, n.trees=500, shrinkage=.05)
test = predict(boost3, CAhousing)
CAhousing = CAhousing %>%
  mutate(prediction1 = test,
         residuals1 =  medianHouseValue - prediction1)
```

```{r echo=FALSE}
################## Plots
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = medianHouseValue))
```
  
In this graph, we see the actual median house value for different areas of California.  
  
```{r echo=FALSE}
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = prediction1))
```
  
This graph shows the predictions of median house values made by our gradient-boosted forest model for houses across different areas of California.  
  
```{r echo=FALSE}
ggmap(CA) +
geom_point(data = CAhousing, mapping = aes(x = longitude, y = latitude, color = residuals1))
```
  
This last graph is the errors of our predictive model, in each area of California, it shows how close to the actual value our prediction was.   



![](main_plot.png)

```{r}
rmse_out_baseline= foreach(i=1:10, .combine='c') %do% {
CAhousing_split<- initial_split(CAhousing, prop= 0.8)
CAhousing_train<- training(CAhousing_split)
CAhousing_test<- testing(CAhousing_split)
baseline_model<- lm(medianHouseValue ~ population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms, data= CAhousing_train)
modelr::rmse(baseline_model, CAhousing_test)
}
##forward selection
  lm0 = lm(medianHouseValue ~ 1, data=CAhousing_train)
  lm_forward = step(lm0, direction='forward',
	scope=~(population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms)^2, trace = 0)
  
rmse_out_forward= foreach(i=1:10, .combine= "c") %do% {
  
CAhousing_split<- initial_split(CAhousing, prop= 0.8)
CAhousing_train<- training(CAhousing_split)
CAhousing_test<- testing(CAhousing_split)
updated_model<- update(lm_forward, data= CAhousing_train)
modelr::rmse(updated_model, CAhousing_test)
}
  
  
##stepwise selection
lm_step = step(baseline_model, 
scope=~(.)^2, trace = 0)
  
rmse_out_step= foreach(i=1:10, .combine= "c") %do% {
  
CAhousing_split<- initial_split(CAhousing, prop= 0.8)
CAhousing_train<- training(CAhousing_split)
CAhousing_test<- testing(CAhousing_split)
updated_model<- update(lm_step, data= CAhousing_train)
modelr::rmse(updated_model, CAhousing_test)
}
##knn 
k_grid = seq(3, 51, by=2)
rmse_knn_out = foreach(i=1:10, .combine='rbind') %dopar% {
  CAhousing_split =  initial_split(CAhousing, prop=0.8)
  CAhousing_train = training(CAhousing_split)
  CAhousing_test  = testing(CAhousing_split)
  this_rmse = foreach(k = k_grid, .combine='c') %do% {
    # train the model and calculate RMSE on the test set
    knn_model = knnreg(medianHouseValue ~ population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms, data=CAhousing_train, k = k, use.all=TRUE)
    modelr::rmse(knn_model, CAhousing_test)
  }
  data.frame(k=k_grid, rmse=this_rmse)
}
rmse_knn_out = arrange(rmse_knn_out, k)
rmse_avg_grid<-rmse_knn_out %>% group_by(k) %>% summarise(avg_rmse= mean(rmse))
best_k= which.min(rmse_avg_grid$avg_rmse)
```


```{r, message=F}
## getting California coast line
osm_box <- getbb (place_name = "California") %>%
  opq(timeout= 70) %>% 
  add_osm_feature("natural", "coastline")
bounds= osm_box$bbox
basemap <- rnaturalearth::ne_countries(scale = 50, country = "united states of america", returnclass = "sf")[1]
basemap_lines <- basemap %>% st_cast("MULTILINESTRING")
ymin <- -32.5295236
xmin <- -124.482003
ymax <-  42.009499
xmax <- -114.1307816
bbox <- st_polygon(list(rbind(c(xmin,ymin), c(xmin,ymax), c(xmax,ymax), c(xmax,ymin),c(xmin,ymin)))) %>% st_sfc()
st_crs(bbox) <- 4326
crop_lines <- st_intersection(basemap_lines, bbox)
plot(crop_lines, main= "California Coastline")
##getting distance from coastline.
CAhousing_sf <- CAhousing %>% st_as_sf(coords = c('longitude','latitude')) 
st_crs(CAhousing_sf)<- 4326
CAhousing_sf$coast_distance<- as.vector(st_distance(crop_lines, CAhousing_sf))
```



```{r}
rmse_out_baseline_coast= foreach(i=1:10, .combine='c') %do% {
CAhousing_sf_split<- initial_split(CAhousing_sf, prop= 0.8)
CAhousing_sf_train<- training(CAhousing_sf_split)
CAhousing_sf_test<- testing(CAhousing_sf_split)
baseline_model_coast<- lm(medianHouseValue ~ population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms + coast_distance, data= CAhousing_sf_train)
modelr::rmse(baseline_model_coast, CAhousing_sf_test)
}
##forward selection
  lm0 = lm(medianHouseValue ~ 1, data=CAhousing_sf_train)
  lm_forward_coast = step(lm0, direction='forward',
	scope=~(population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms + coast_distance)^2, trace = 0)
  
rmse_out_forward_coast= foreach(i=1:10, .combine= "c") %do% {
  
CAhousing_sf_split<- initial_split(CAhousing_sf, prop= 0.8)
CAhousing_sf_train<- training(CAhousing_sf_split)
CAhousing_sf_test<- testing(CAhousing_sf_split)
updated_model<- update(lm_forward_coast, data= CAhousing_sf_train)
modelr::rmse(updated_model, CAhousing_sf_test)
}
  
  
##stepwise selection
lm_step_coast = step(baseline_model_coast, 
scope=~(.)^2, trace = 0)
  
rmse_out_step_coast= foreach(i=1:10, .combine= "c") %do% {
  
CAhousing_sf_split<- initial_split(CAhousing_sf, prop= 0.8)
CAhousing_sf_train<- training(CAhousing_sf_split)
CAhousing_sf_test<- testing(CAhousing_sf_split)
updated_model<- update(lm_step_coast, data= CAhousing_sf_train)
modelr::rmse(updated_model, CAhousing_sf_test)
}
##knn
k_grid = seq(3, 51, by=2)
rmse_knn_out_coast = foreach(i=1:10, .combine='rbind') %dopar% {
  CAhousing_sf_split =  initial_split(CAhousing_sf, prop=0.8)
  CAhousing_sf_train = training(CAhousing_sf_split)
  CAhousing_sf_test  = testing(CAhousing_sf_split)
  this_rmse = foreach(k = k_grid, .combine='c') %do% {
    # train the model and calculate RMSE on the test set
    knn_model_coast = knnreg(medianHouseValue ~ population + housingMedianAge + medianIncome + avg_rooms + avg_bedrooms+ coast_distance, data=CAhousing_sf_train, k = k, use.all=TRUE)
    modelr::rmse(knn_model_coast, CAhousing_sf_test)
  }
  data.frame(k=k_grid, rmse=this_rmse)
}
rmse_knn_out_coast = arrange(rmse_knn_out_coast, k)
rmse_avg_grid_coast<-rmse_knn_out_coast %>% group_by(k) %>% summarise(avg_rmse= mean(rmse))
best_k_coast= which.min(rmse_avg_grid_coast$avg_rmse)
```


\newpage

3) Results

The figure below shows the root mean squared error for our different model types. We see that the models with the distance to coast variable perform significantly better for all model types. There is not a large difference in terms of out of sample accuracy between the forward selection model and the stepwise model. We use the stepwise model as our model of choice. 

```{r}
values<-c(mean(rmse_out_baseline_coast), mean(rmse_out_forward_coast), mean(rmse_out_step_coast), rmse_avg_grid_coast[best_k, ]$avg_rmse)
model_type<- c("baseline","forward", "step", "knn")
coast<- rep("coast", 4)
rmse_coast= data.frame(values, model_type, coast)
values<-c(mean(rmse_out_baseline), mean(rmse_out_forward), mean(rmse_out_step), rmse_avg_grid[best_k, ]$avg_rmse)
model_type<- c("baseline","forward", "step", "knn")
coast<- rep("no coast", 4)
rmse_no_coast= data.frame(values, model_type, coast)
rmse_df= rbind(rmse_coast, rmse_no_coast)
ggplot(rmse_df, aes(fill=coast, y=values, x=model_type)) + 
    geom_bar(position="dodge", stat="identity")+ labs(y= "RMSE", x= "Model Type")+
  theme_minimal()
```



```{r}
rmse_best<- mean(rmse_out_step_coast)
print(paste("The model's RMSE is ", rmse_best), quote= F)
```


We then use this model to predict median house values using our original dataset and we get the absolute value of the prediction errors from this model as well. The plot below shows these on a map of California

```{r}
CAhousing_sf$pred<-predict(lm_step_coast, CAhousing_sf)
CAhousing_sf$error<- abs(CAhousing_sf$medianHouseValue - CAhousing_sf$pred)
CAhousing$pred= CAhousing_sf$pred
CAhousing$error= CAhousing_sf$error
plot1<-ggplot(data= ca_tracts)+
  geom_sf()+
  geom_point(data=CAhousing, aes(x=longitude, y=latitude, color= pred/1000))+
  coord_sf()+
  scale_color_viridis(option = "D")+
  theme_minimal()+ labs(title= "Predicted Median House Value in California (in 000s of $USD)", color= "Predicted Median House Values") 
ggsave("pred_plot.png", width = 13, #long width
        height =7)
plot2<-ggplot(data= ca_tracts)+
  geom_sf()+
  geom_point(data=CAhousing, aes(x=longitude, y=latitude, color= error/1000))+
  coord_sf()+
  scale_color_viridis(option = "D")+
  theme_minimal()+ labs(title= "Residuals from Predicted Median House Values in California (in 000s of $USD)", color= "Residuals") 
ggsave("resid_plot.png", width = 13, #long width
        height =7)
```


